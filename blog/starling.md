---
title: "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF"
author: "Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao"
date: "Nov 30, 2023"
previewImg: starling.png
---

We introduce Starling-7B, an open-source chatbot model trained by RLAIF with our new GPT-4 labeled ranking dataset, Nectar, and our new reward training + policy tuning pipeline. Starling-LM-7B-alpha achieves 8.09 in MT Bench with GPT-4 as a judge, outcompeting every model to date on MT-Bench except for OpenAI's GPT-4 and GPT-4 Turbo. We release the ranking dataset [Nectar](https://huggingface.co/berkeley-nest/nector), the reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and the language model [Starling-LM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) on HuggingFace, and an online demo in [chatbot arena](https://chat.lmsys.org) for non-commercial use. The code and paper providing more details will come out soon.

<img src="starling.png" style="width: 30%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img>
<p style="color:gray; text-align: center;">Starling-LM-7B (generated by DALL·E 3) </p>

<img src="mt-bench-score.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*According to MT Bench, an evaluation benchmark based on GPT-4 scoring. Further human evaluation is needed.</p>

## Overview

Supervised fine-tuning (SFT) has seen great success in training language models as chatbot systems, especially with high quality data distilled from ChatGPT / GPT 4 (see e.g. Alpaca, Vicuna, OpenHermes 2.5, Openchat 3.5). However, it remains unclear how much gain RLHF can bring when scaling high-quality preference data. Initial attempt in open source community applies Direct Policy Optimization (DPO) to get models like Zephyra-7B, Neural-Chat-7B or Tulu-70B. But from the score in MT Bench and Chatbot arena,  compared with the best SFT model like OpenHermes 2.5, Openchat 3.5, their performance is still incapable of demonstrating the power of RLHF.

To faciliate rigorous research on RLHF, one needs a ranking dataset that is as high quality as the existing best SFT dataset. We release Nectar, a GPT-4 labeled ranking dataset composed of 183K chat prompts, with 7 responses each from GPT-4, GPT-3.5-instruct, GPT-3.5, Mistral-7B-Instruct, Llama2-7B and other models, resulting in total 3.8M pairwise comparisons. We spent a good amount of prompting efforts to remove the positional bias when asking GPT-4 to provide ranking for 7 responses. The details are provided in the dataset section below.

Furthermore, to the best of our knowledge, there are not too many choices of open-source reward models. We also open source our reward model trained with our proposed K-wise loss from the Nectar dataset. We hope this reward model can help people better understand the RLHF mechanism, and potentially benefit the AI safety research.

Lastly, we fine-tune the existing language model Openchat-3.5 based on the learned reward model. We observe that the MT-Bench score improves from 7.81 to 8.09, and the AlpacaEval score improves from 88.51% to 91.99%. Both datasets are designed to measure the chat experience in helpfulness. We also observe slight regression in the model's general capability, potentially due to slight overfitting. We are actively testing different methodologies for training the reward model and language model, and will keep updating the blog and our released models. 



##  Evaluation of the Model

Evaluating chatbots is never a simple task. We mainly evaluate the helpfulness of our models based on MT-Bench and AlpacaEval, which are GPT-4-based comparisons. The results are listed below.

| Model                 | Tuning Method    | MT Bench | AlpacaEval |
|-----------------------|------------------|----------|------------|
| gpt4_turbo            | ?                | 9.32     | 97.70      |
| gpt4                  | SFT + PPO        | 8.99     | 95.28      |
| Starling-7B           | C-RLFT + APA     | 8.09     | 91.99      |
| claude-2              | ?                | 8.06     | 91.36      |
| GPT-3.5               | ?                | 7.94     | 89.37      |
| claude-1              | ?                | 7.9      | 88.39      |
| tulu-2-dpo-70b        | SFT + DPO        | 7.89     | 95.1       |
| Openchat-3.5          | C-RLFT           | 7.81     | 88.51      |
| Zephyr-7B-beta        | SFT + DPO        | 7.34     | 90.60      |
| llama-2-70b-chat-hf   | SFT + PPO        | 6.86     | 92.66      |
| neural-chat-7b-v3-1   | SFT + DPO        | 6.84     | 84.53      |
| Tulu-2-dpo-7b         | SFT + DPO        | 6.29     | 85.1       |


We also put our model anonymously on Chatbot Arena, where we test the human preferences. The result is XXX (waiting for Chatbot Arena scores in 1 week.)

Similar to the observations in GPT-4 report, we also noticed that after RLHF, the helpfulness and safety of the model improves while the other capabilities do not change too much. We also observe a similar phenomenon that after initial RLHF, the model can be unnecessarily safe towards some non-malicious prompts, and can still be unsafe to jailbreaking prompts. We also want to point out that due to Goodharting law, the model being preferred by GPT-4 may not indicate that it is more preferred by human than any model whose MT-Bench score is lower. The model is also limited due to its main capability, including basic knowledge, reasoning, coding and mathematics, and RLHF only improves its helpfulness and harmlessness, which is reflected in MT-Bench and AlpacaEval. However, we believe the result suggests the possibility of scaling online RL methods with massive preference data -- when the ground truth is GPT-4 preference, it is possible to imporove over the best existing models. If one changes the preference data with high-quality human responses, it's likely to improve human preferences as well. 


## Dataset

We provide the first high-quality 7-wise comparison dataset generated by GPT-4-based ranking. The prompts are mainly subsampled from a wide-range of datasets, including lmsys-chat-1M, ShareGPT, Antropic/hh, UltraFeedback, Evol-Instruct and Flan. The 7 responses include mainly response we distilled from GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, Mistral-7B-Instruct.  Responses were also sourced from other existing datasets and models. 

<img src="rlaif_dataset.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*The process of generating Nectar, a 7-wise comparison dataset for RLHF.</p>

The most difficult part is the positional bias in GPT-4-based ranking. We plot the probability of being picked as the top response for all positions in the ranking prompt.  As we show in the first figure below, when we simply ask GPT-4 to rank the responses without additional reasoning, GPT-4 prefers the responses in the first and second place much more than responses in the last two positions. In the second figure, we explicitly ask GPT-4 to first provide pairwise comparisons for all pairs, and then summarize with 7-wise ranking. We find that helps mitigate the positional bias a bit. We have also tried asking GPT-4 to provide scores or judgement for each individual prompt, and then summarize with 7-wise ranking. Unfortunately, this does not help with reducing positional bias. In the third figure, we provide a specific tie breaking order and find that it helps further reduce the positional bias. In the last figure, we enforce a randomized tie breaking order. We find this helps the most in terms of reducing positional bias. This gives the final prompt we use for curating the dataset Nectar.

We hope this dataset can help developers train better models with RLHF, and help researchers develop better understanding in RLHF, along with the relationship between synthetic data and human data.

## RLHF / RLAIF

After getting the Nectar dataset, we use it to train a reward model and conduct online RL based on that. We provide the detailed process as below. 

<img src="rlaif_dataset.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*The process of RLHF / RLAIF.</p>

Our reward model is trained with the K-wise maximum likelihood estimator under Plackett-Luce Model, as is shown in the [prior paper [1]]{https://arxiv.org/abs/2301.11270}. We observe that for 7-wise comparisons, the new estimator gives better reward model than the original loss proposed for training reward model, which first splits the comparisons into pairwise, and then minimizes the cross entropy loss for pairwise comparisons. We are actively experimenting with new loss functions for training a better reward model. 

For policy-finetuning, we select the initial model Openchat 3.5, since the model has the highest MT Bench score (7.81) among all existing models. It will be more convincing if RLHF can further improve the score in MT Bench. We tested three online RL methods, Advantage-induced Policy Alignment (APA), Proximal Policy Optimization (PPO) and Pairwise Proximal Policy Optimization (P3O). In general, the search of optimal hyperpamameters for PPO can be hard. But when optimized with god hyperparameters, we find that existing online RL methods give similarly good performances. We select the final checkpoint from an APA run. We have also tried offline-RL methods including [Direct Preference Optimization (DPO)]{https://arxiv.org/abs/2305.18290}. However, there is no strong evidence yet showing that it can bring substantial improvement over the SFT model Openchat 3.5, potentially due to that Openchat 3.5 is already fine-tuned with offline RL, and the performance of offline RL is usually the best covered demonstration policy. In the future, we envision a better language model fine-tuning procedure being using (conditional) offline RL including DPO or C-RLFT to leverage reward information to create a strong initial model, and further improve the helpfulness and harmlessness with reward training and online RL. 

During the process, we find that compared with the policy tuning method, the quality of the reward model plays a more important role. We call for new and most robust reward learning methods and encourage researchers to train and test the reward model on our dataset. We believe it's likely that the dataset can bring higher gain with a larger reward model and language model.


### Evaluation of RLHF

Evaluating the performance of RLHF algorithms can be hard, especially when it becomes less clear whether the performance gain comes from imitating the best demonstration (GPT-4) responses or extrapolating to better responses. We believe the main benefit of RLHF is to further improve the performance when learning from demonstration data cannot further boost the performance. Thus we encourage researchers to test out their RLHF algorithms on our dataset and start with the best existing model that learns from demonstrations (e.g. Openchat 3.5). The best testbench is to produce strong models that go beyond demonstration data. 

On the other hand, training reward model on GPT-4 preference data and testing on GPT-4-based scoring can create double layers of impact from Goodharting laws. First, GPT-4-based preference is a proxy of human preferences. Overoptimizing towards GPT-4 preference may harm the actual human preference. Second, the learned reward model is only a proxy of the GPT-4-based preference, thus overoptimizing the proxy reward predicted by the reward model can hurt the GPT-4 preference, which may in turn also hurt human preferences. How to better utilize synthetic preference data to mitigate the two layer Goodharting law, and how to evaluate the model without too much human involvement remains an open problem. 


## Limitations
We have noticed that, similar to other small-sized LLMs, Starling-7B has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. The model is likely to be unsafe to jailbreaking prompts, since it is not explicitly trained on these. We also observe that in rare cases, the model may output verbose and unnecessary content. We are working to release a better checkpoint when exploring new reward training and policy training methods. We also call for the community to join us or collaborate with us to train better models with RLHF.



## Online Demo
Try the Starling-7B demo [here](https://chat.lmsys.org)!

<!-- Add a video that automatically play -->
<div>
  <a href="https://chat.lmsys.org"  style="display: flex; justify-content: center; margin-top: 1em; margin-bottom: 1em;">
  <video autoplay muted loop src="/images/blog/vicuna/demo-narrow.mp4" type="video/mp4" style="width: 70%;" id="demo">
  </video>
  </a>
</div>


## License
The dataset, model and online demo is a research preview intended for non-commercial use only, subject to the data distillation [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.


## Acknowledgment
We would like to thank the lmsys org for their support of evaluation and online demo. 

**✉ Correspondence to:** Banghua Zhu (banghua@berkeley.edu).

## Citation
```
@misc{starling2023,
    title = {title: "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF},
    url = {},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}
```
