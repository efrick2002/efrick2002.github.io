---
title: "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF"
author: "Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao"
date: "Nov 30, 2023"
previewImg: starling.png
---

We introduce Starling-7B, an open-source large language model (LLM) trained by Reinforcement Learning with AI Feedback (RLAIF). The model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI's GPT-4 and GPT-4 Turbo. We release the ranking dataset [Nectar](https://huggingface.co/berkeley-nest/nector), the reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and the language model [Starling-LM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) on HuggingFace, and an online demo in [chatbot arena](https://chat.lmsys.org) for non-commercial use. Stay tuned for our forthcoming code and paper, which will provide more details on the whole process.

<img src="starling.png" style="width: 30%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img>
<p style="color:gray; text-align: center;">Starling-LM-7B (generated by DALL·E 3) </p>

<img src="mt-bench-score.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*Based on MT Bench evaluations, using GPT-4 scoring. Further human evaluation is needed.</p>

## Overview

Supervised fine-tuning (SFT) has demonstrated remarkable effectiveness in developing chatbot systems from language models, particularly when leveraging high-quality data distilled from ChatGPT/GPT-4 (examples include Alpaca, Vicuna, OpenHermes 2.5, and Openchat 3.5).  However, the extent to which Reinforcement Learning from Human Feedback (RLHF) can enhance models when scaling high-quality preference data remains an open question. Earlier endeavors in the open-source community, such as Zephyra-7B, Neural-Chat-7B, and Tulu-70B, employed Direct Policy Optimization (DPO), but their performance in MT Bench (and some in Chatbot Arena), when compared to leading SFT models like OpenHermes 2.5 and Openchat 3.5, has not fully showcased RLHF's potential.

To facilitate more thorough research into RLHF, a high-quality ranking dataset specifically for chat is essential. We release Nectar, a GPT-4 labeled ranking dataset composed of 183K chat prompts. Each prompt includes 7 responses distilled from various models like GPT-4, GPT-3.5-instruct, GPT-3.5-turbo, Mistral-7B-Instruct, Llama2-7B, resulting in a total of 3.8M pairwise comparisons. Considerable effort was invested in mitigating positional bias when prompting GPT-4 for rankings, the details of which are elaborated in the dataset section below.

Moreover, there is a notable scarcity of open-source reward models. We address this gap by releasing our reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha), trained with our K-wise loss on the Nectar dataset. 

Lastly, we fine-tuned the Openchat-3.5 language model using the learned reward model. This resulted in an increase in the MT-Bench score from 7.81 to 8.09, and an improvement in the AlpacaEval score from 88.51% to 91.99%. Both metrics assess the chatbot's helpfulness. 

We hope the open-sourced dataset, reward model and language model can help deepen the understanding of the RLHF mechanism and contribute to AI safety research. Our team is actively exploring various training methodologies for both the reward and language models, and we will continue to update this blog with our findings and model releases.


##  Evaluation of the Model

Evaluating chatbots is never a simple task. We mainly evaluate the helpfulness of our models based on MT-Bench and AlpacaEval, which are GPT-4-based comparisons. The results are listed below.

| Model                 | Tuning Method    | MT Bench | AlpacaEval |
|-----------------------|------------------|----------|------------|
| gpt4_turbo            | ?                | 9.32     | 97.70      |
| gpt4                  | SFT + PPO        | 8.99     | 95.28      |
| Starling-7B           | C-RLFT + APA     | 8.09     | 91.99      |
| claude-2              | ?                | 8.06     | 91.36      |
| GPT-3.5               | ?                | 7.94     | 89.37      |
| claude-1              | ?                | 7.9      | 88.39      |
| tulu-2-dpo-70b        | SFT + DPO        | 7.89     | 95.1       |
| Openchat-3.5          | C-RLFT           | 7.81     | 88.51      |
| Zephyr-7B-beta        | SFT + DPO        | 7.34     | 90.60      |
| llama-2-70b-chat-hf   | SFT + PPO        | 6.86     | 92.66      |
| neural-chat-7b-v3-1   | SFT + DPO        | 6.84     | 84.53      |
| Tulu-2-dpo-7b         | SFT + DPO        | 6.29     | 85.1       |


We also put our model anonymously on Chatbot Arena, where we test the human preferences. The result is XXX (waiting for Chatbot Arena scores in 1 week.)

In line with findings in GPT-4 Technical Report, our observations post-RLHF reveal similar trends. We've noted an enhancement in the model's helpfulness and safety, while other capabilities remain largely unaffected. We also detected a tendency for the model to respond with excessive caution to certain benign prompts after initial RLHF, while still showing vulnerabilities to jailbreaking attempts. This may require further fine-tuning with rule-based reward models with GPT-4 as classifiers, similar to what is done in GPT-4 Technical Report.

It's important to highlight that the model's preference ranking by GPT-4 does not necessarily correlate with human preference, a phenomenon that echoes the principles of Goodhart's Law. Essentially, a higher MT-Bench score, as endorsed by GPT-4, doesn't automatically imply greater human favorability, especially compared to models with lower scores. The core competencies of the model, encompassing basic knowledge, reasoning, coding, and mathematics, remain unchanged. RLHF primarily enhances aspects of helpfulness and safety, as evidenced in its performance in MT-Bench and AlpacaEval. However, these results do hint at the potential of scaling online RL methods using extensive preference data. When the benchmark is GPT-4's preferences, surpassing the performance of existing models is feasible. Moreover, adapting the preference data to include high-quality human responses could likely lead to improvements in aligning with human preferences.



## Dataset Overview

We present Nectar, the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. The dataset's prompts are an amalgamation of diverse sources, including lmsys-chat-1M, ShareGPT, Antropic/hh, UltraFeedback, Evol-Instruct, and Flan. Responses are primarily derived from a variety of models, namely GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, and Mistral-7B-Instruct, alongside other existing datasets and models.


<img src="rlaif_dataset.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*Illustrating the creation process of Nectar, a 7-wise comparison dataset for RLAIF.</p>

The most challenging aspect of creating Nectar was mitigating the positional bias inherent in GPT-4-based rankings. We extensively analyzed the likelihood of a response being selected as the top choice based on its position in the ranking prompt. Our initial findings, depicted in the first figure below, revealed a significant bias towards responses in the first and second positions when GPT-4 was simply asked to rank responses without additional reasoning.


<img src="position_bias.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*The positional bias of  GPT-4-based ranking.</p>

To address this, as shown in the second figure, we instructed GPT-4 to first conduct pairwise comparisons for all response pairs before compiling a 7-wise ranking. This approach moderately reduced the positional bias. We have also explored having GPT-4 score or judge each prompt individually before summarizing in a 7-wise ranking, but this method did not effectively diminish the bias.

Further reduction of positional bias came with the introduction of a specific, and then a randomized, tie-breaking order, as demonstrated in the third and fourth figures, respectively. This approach proved most effective in counteracting positional bias, leading to the final methodology employed in curating the Nectar dataset.

We believe that Nectar will be a valuable resource for developers aiming to train more effective models using RLHF / RLAIF. It also offers high-quality responses for a diverse range of prompts, and can provide researchers with deeper insights into RLHF / RLAIF and the interplay between synthetic and human data.



## RLHF / RLAIF 

We train a reward model and conducting online RL based on the existing Nectar Dataset. Detailed below is our process, illustrated for clarity.

<img src="rlhf.png" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img>

<p style="color:gray;">*Illustrating the RLHF / RLAIF process.</p>
Our reward model leverages the K-wise maximum likelihood estimator under the Plackett-Luce Model, as detailed in [prior paper](https://arxiv.org/abs/2301.11270). We discovered that for 7-wise comparisons, this new estimator yields a more effective reward model than the original loss, which converts comparisons into pairwise and minimizes cross-entropy loss.

We selected Openchat 3.5 as the initial model for policy-finetuning, owing to its high MT Bench score (7.81). Our objective was to ascertain whether RLHF could enhance this score further. We experimented with three online RL methods: Advantage-induced Policy Alignment (APA), Proximal Policy Optimization (PPO), and Pairwise Proximal Policy Optimization (P3O). Despite the challenges in hyperparameter optimization for PPO, we found that, with optimal settings, these methods yielded comparably strong results. We ultimately selected a checkpoint from an APA run. Our trials with offline RL methods like Direct Preference Optimization (DPO) showed no significant improvements over the SFT model Openchat 3.5. This is likely due to that Openchat 3.5 has already done a different format of offline preference-based training. In the future, we envision a better language model fine-tuning procedure being using (conditional) offline RL including DPO or C-RLFT to leverage reward information to create a strong initial model, and further improve the helpfulness and harmlessness with reward training and online RL. 

We observed that the quality of the reward model significantly influences the results, more so than the policy tuning method itself. We encourage the development of robust reward learning methods and invite researchers to utilize our dataset for training and testing. We believe it's likely that the dataset can bring higher gain with a larger reward model and language model, according to the [scaling laws of the reward model](https://arxiv.org/abs/2210.10760).

### Evaluation of RLHF

Evaluating RLHF algorithms presents unique challenges, particularly in discerning whether performance gains are due to imitation of the best demonstration policies in offline-RL-based methods or innovative extrapolations in online-RL-based methods. We advocate for testing RLHF algorithms on our dataset, starting with models already proficient in learning from demonstrations, like Openchat 3.5. The ultimate benchmark should be the creation of models that surpass the initial model in both GPT-4 and human preferences.

However, training on GPT-4 preference data and evaluating against GPT-4-based scoring may invoke double layers of impact from Goodhart's laws. Over-optimization towards GPT-4 preferences could inadvertently harm actual human preferences. Similarly, the reward model, being a proxy for GPT-4 preferences, might also misalign with human preferences when over-optimized. The challenge lies in effectively utilizing synthetic preference data to mitigate these issues and evaluating models with minimal human intervention.


## Limitations
Starling-7B, akin to other small-sized LLMs, has its limitations. It struggles with tasks involving reasoning or mathematics and may not always accurately self-identify or ensure the factual correctness of its outputs. Additionally, it's susceptible to jailbreaking prompts, as it wasn't explicitly trained for these scenarios. Occasionally, the model may generate verbose or unnecessary content. We are committed to improving Starling-7B, exploring new reward training, and policy training methods. We invite the community to collaborate with us in this endeavor to enhance models with RLHF.



## Online Demo
Try the Starling-7B demo [at lmsys](https://chat.lmsys.org)!

<!-- Add a video that automatically play -->
<div>
  <a href="https://chat.lmsys.org"  style="display: flex; justify-content: center; margin-top: 1em; margin-bottom: 1em;">
  <video autoplay muted loop src="/images/blog/vicuna/demo-narrow.mp4" type="video/mp4" style="width: 70%;" id="demo">
  </video>
  </a>
</div>


## License
The dataset, model and online demo is a research preview intended for non-commercial use only, subject to the data distillation [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.


## Acknowledgment
We would like to thank the lmsys org for their support of evaluation and online demo. 

**✉ Correspondence to:** Banghua Zhu (banghua@berkeley.edu).

## Citation
```
@misc{starling2023,
    title = {title: "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF},
    url = {},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}
```
