---
title: "Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF"
author: "Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao"
date: "Nov 30, 2023"
previewImg: starling.png
---

We introduce Starling-7B, an open-source chatbot model trained by RLAIF with our new GPT-4 labeled ranking dataset, Nectar, and our new reward training + policy tuning pipeline. Starling-LM-7B-alpha achieves 8.09 in MT Bench with GPT-4 as a judge, outcompeting every model to date on MT-Bench except for OpenAI's GPT-4 and GPT-4 Turbo. We release the ranking dataset [Nectar](https://huggingface.co/berkeley-nest/nector), the reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and the language model [Starling-LM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) on HuggingFace, and an online demo in [chatbot arena](https://chat.lmsys.org) for non-commercial use. The code and paper providing more details will come out soon.

<img src="starling.png" style="width: 30%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img>
<p style="color:gray; text-align: center;">Starling-LM-7B (generated by DALL·E 3) </p>

<img src="mt-bench-score.png" style="width: 30%; margin-left: auto; margin-right: auto; margin-bottom: auto"></img> 
<p style="color:gray;">*According to MT Bench, an evaluation benchmark based on GPT-4 scoring. Further human evaluation is needed.</p>

## Overview

Supervised fine-tuning (SFT) has seen great success in training language models as chatbot systems, especially with high quality data distilled from ChatGPT / GPT 4 (see e.g. Alpaca, Vicuna, OpenHermes 2.5, Openchat 3.5). However, it remains unclear how much gain RLHF can bring when scaling high-quality preference data. Initial attempt in open source community applies Direct Policy Optimization (DPO) to get models like Zephyra-7B, Neural-Chat-7B or Tulu-70B. However, from the score in MT Bench and Chatbot arena,  compared with the best SFT model like OpenHermes 2.5, Openchat 3.5, their performance is still incapable of demonstrating the power of RLHF.

To faciliate rigorous research on RLHF, one needs a ranking dataset that is as high quality as the existing best SFT dataset. We release Nectar, a GPT-4 labeled ranking dataset composed of 183K chat prompts, with 7 responses each from GPT-4, GPT-3.5-instruct, GPT-3.5, Mistral-7B-Instruct, Llama2-7B and other models, resulting in total 3.8M pairwise comparisons. We spent a good amount of prompting efforts to remove the positional bias when asking GPT-4 to provide ranking for 7 responses. The details are provided in the dataset section below.

Furthermore, to the best of our knowledge, there are not too many choices of open-source reward models. We also open source our reward model trained with our proposed K-wise loss from the Nectar dataset. We hope this reward model can help people better understand the RLHF mechanism, and potentially benefit the AI safety research.

Lastly, we fine-tune the existing language model Openchat-3.5 based on the learned reward model. We observe that the MT-Bench score improves from 7.81 to 8.09, and the AlpacaEval score improves from 88.51% to 91.99%. Both datasets are designed to measure the chat experience in helpfulness. We also observe slight regression in the model's general capability, potentially due to slight overfitting. We are actively testing different methodologies for training the reward model and language model, and will keep updating the blog and our released models. 


## Dataset

We provide the first high-quality 7-wise comparison dataset generated by GPT-4.  The 7 responses include mainly response we distilled from GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, Mistral-7B-Instruct.  Responses were also sourced from other existing datasets. 
Starling is trained on our new dataset Nectar: the first open source RLAIF dataset composed of 183k prompts with *7* rated responses each.

## RLHF / RLAIF


Starling is fine-tuned based on learned GPT-4 preferences, which we find helpful for further improving its helpfulness and harmlessness in chat scenario. 

## Evaluation


However, evaluating chatbots is never a simple task. 
With recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework for benchmark generation and performance assessments. 
Our initial finding indicates that GPT-4 can produce highly consistent ranks and detailed assessment when comparing chatbots’ answers (see above example of GPT-4 judgment).
Preliminary evaluations based on GPT-4, summarized in Figure 1, show that Vicuna achieves 90%<sup>*</sup> capability of Bard/ChatGPT. 
While this proposed framework shows a potential to automate chatbot assessment, **it is not yet a rigorous approach**. 
Building an evaluation system for chatbots remains an open question requiring further research. More details are provided in the evaluation section.

<img src="/images/blog/vicuna/chart.svg" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%"></img>
<p style="color:gray; text-align: center;">Figure 1. Relative Response Quality Assessed by GPT-4*</p>

<img src="/images/blog/vicuna/overview.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%"></img>
<p style="color:gray; text-align: center;">Figure 2. Workflow Overview</p>

Figure 2 provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-turn conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provides better responses. A detailed comparison of LLaMA, Alpaca, ChatGPT, and Vicuna is shown in Table 1 below.


<p style="color:gray; text-align: center;">Table 1. Comparison between several notable models</p>

<table class="tg" style="display: flex;justify-content: center;">
<tbody>
  <tr>
    <td class="tg-head"><span style="font-weight:bold;">Model Name</span></td>
    <td class="tg-head"><span style="font-weight:bold;">LLaMA</span></td>
    <td class="tg-head"><span style="font-weight:bold;">Alpaca</span></td>
    <td class="tg-head"><span style="font-weight:bold;">Vicuna</span></td>
    <td class="tg-head"><span style="font-weight:bold;">Bard/ChatGPT</span></td>
  </tr>
  <tr>
    <td class="tg-body">Dataset</td>
    <td class="tg-body">Publicly available datasets<br>(1T token)</td>
    <td class="tg-body">Self-instruct from davinci-003 API<br>(52K samples)</td>
    <td class="tg-body">User-shared conversations<br>(70K samples)</td>
    <td class="tg-body">N/A</td>
  </tr>
  <tr>
    <td class="tg-body">Training code</td>
    <td class="tg-body">N/A</td>
    <td class="tg-body">Available</td>
    <td class="tg-body">Available</td>
    <td class="tg-body">N/A</td>
  </tr>
  <tr>
    <td class="tg-body">Evaluation metrics</td>
    <td class="tg-body">Academic benchmark</td>
    <td class="tg-body">Author evaluation</td>
    <td class="tg-body">GPT-4 assessment</td>
    <td class="tg-body">Mixed</td>
  </tr>
  <tr>
    <td class="tg-body">Training cost<br>(7B)</td>
    <td class="tg-body">82K GPU-hours</td>
    <td class="tg-body">$500 (data) + $100 (training)</td>
    <td class="tg-body">$140 (training)</td>
    <td class="tg-body">N/A</td>
  </tr>
  <tr>
    <td class="tg-body">Training cost<br>(13B)</td>
    <td class="tg-body">135K GPU-hours</td>
    <td class="tg-body">N/A</td>
    <td class="tg-body">$300 (training)</td>
    <td class="tg-body">N/A</td>
  </tr>
</tbody>
</table>



## Evaluation of the Model


## How To Evaluate the Performance of RLHF Algorithms?

Evaluating AI chatbots is a challenging task, as it requires examining language understanding, reasoning, and context awareness. With AI chatbots becoming more advanced, current open benchmarks may no longer suffice. For instance, the evaluation dataset used in Stanford’s Alpaca, [self-instruct](https://github.com/yizhongw/self-instruct/tree/main/human_eval), can be effectively answered by SOTA chatbots, making it difficult for humans to discern differences in performance. More limitations include training/test data contamination and the potentially high cost of creating new benchmarks. To tackle these issues, we propose an evaluation framework based on GPT-4 to automate chatbot performance assessment.

First, we devised eight question categories, such as Fermi problems, roleplay scenarios, and coding/math tasks, to test various aspects of a chatbot's performance. Through careful prompt engineering, GPT-4 is able to generate diverse, challenging questions that baseline models struggle with. We select ten questions per category and collect answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. We then ask GPT-4 to rate the quality of their answers based on helpfulness, relevance, accuracy, and detail. We discover that GPT-4 can produce not only relatively consistent scores but also detailed explanations on why such scores are given (detailed examples [link](https://lmsys.org/vicuna_eval/)). However, we also notice that GPT-4 is not very good at judging coding/math tasks.

<img src="/images/blog/vicuna/response-compare.png" style="display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%;"></img>
<p style="color:gray; text-align: center;">Figure 3. Response Comparison Assessed by GPT-4</p>

Figure 3 displays the comparison results between all baselines and Vicuna. GPT-4 prefers Vicuna over state-of-the-art open-source models (LLaMA, Alpaca) in more than 90% of the questions, and it achieves competitive performance against proprietary models (ChatGPT, Bard). In 45% of the questions, GPT-4 rates Vicuna's response as better or equal to ChatGPT's.
As GPT-4 assigns a quantitative score to each response on a scale of 10, we calculate the total score for each (baseline, Vicuna) comparison pair by adding up the scores obtained by each model on 80 questions. As shown in Table 2, Vicuna’s total score is 92% of ChatGPT’s. Despite recent advancements, these chatbots still face limitations, such as struggling with basic math problems or having limited coding ability.

<p style="color:gray; text-align: center;">Table 2. Total Scores Assessed by GPT-4. </p>

<table class="tg" style="display: flex;justify-content: center;">
<tbody>
  <tr>
    <td class="tg-head"><span style="font-weight:bold;">Baseline</span></td>
    <td class="tg-head"><span style="font-weight:bold;">Baseline Score</span></td>
    <td class="tg-head"><span style="font-weight:bold;">Vicuna Score</span></td>
  </tr>
  <tr>
    <td class="tg-body">LLaMA-13B</td>
    <td class="tg-body" style="text-align: right">513.0</td>
    <td class="tg-body" style="text-align: right"><span style="font-weight:bold;">694.0</span></td>
  </tr>
  <tr>
    <td class="tg-body">Alpaca-13B</td>
    <td class="tg-body" style="text-align: right">583.0</td>
    <td class="tg-body" style="text-align: right"><span style="font-weight:bold;">704.0</span></td>
  </tr>
  <tr>
    <td class="tg-body">Bard</td>
    <td class="tg-body" style="text-align: right"><span style="font-weight:bold;">664.0</span></td>
    <td class="tg-body" style="text-align: right">655.5</td>
  </tr>
  <tr>
    <td class="tg-body">ChatGPT</td>
    <td class="tg-body" style="text-align: right"><span style="font-weight:bold;">693.0</span></td>
    <td class="tg-body" style="text-align: right">638.0</td>
  </tr>
</tbody>
</table>
<br>

While this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach, as large language models are prone to hallucinate. Developing a comprehensive, standardized evaluation system for chatbots remains an open question requiring further research.

**Edited**: After this blog post, we conducted a deeper study on this GPT4-based evaluation approach. You are welcome to read our new [Judging LLM-as-a-judge paper](https://arxiv.org/abs/2306.05685) and try the new evaluation [tool](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge).

## Limitations
We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

## Release
In our first release, we will share the training, serving, and evaluation code on a GitHub repo: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat).
We also released the Vicuna-13B model [weights](https://github.com/lm-sys/FastChat#vicuna-weights).
There is no plan to release the dataset. Join our [Discord](https://discord.gg/HSWAKCrnFx) server and follow our [Twitter](https://twitter.com/lmsysorg) to get the latest updates.


## Online Demo
Try the Vicuna-13B demo [here](https://chat.lmsys.org)!

<!-- Add a video that automatically play -->
<div>
  <a href="https://chat.lmsys.org"  style="display: flex; justify-content: center; margin-top: 1em; margin-bottom: 1em;">
  <video autoplay muted loop src="/images/blog/vicuna/demo-narrow.mp4" type="video/mp4" style="width: 70%;" id="demo">
  </video>
  </a>
</div>


## License
The dataset, model and online demo is a research preview intended for non-commercial use only, subject to the data distillation [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.


## Acknowledgment
We would like to thank the lmsys org for their support of evaluation and online demo.  Xuecheng Li, and Tianyi Zhang from Stanford Alpaca team for their insightful discussion and feedback; Qirong Ho from MBZUAI for providing support on the serving cluster. Please check out a blog post from BAIR about a concurrent effort on their chatbot, [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/).

**✉ Correspondence to:** Banghua Zhu (lianminzheng@gmail.com), Hao Zhang (sjtu.haozhang@gmail.com), or LMSYS (lmsys.org@gmail.com).

## Citation
```
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
```
