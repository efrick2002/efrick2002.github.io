# Starling: Increasing LLM performance with RLAIF
## By: some cool researchers

Starling-LM is our new high performance AI assistant chat model that outcompetes every model on MT-Bench except OpenAI's GPT-4.  Starling is trained on our new dataset Nectar: the first open source RLAIF dataset composed of 180k prompts with *7* rated responses each.  Starling-RM is our reward model trained on the Nectar dataset. Starling-LM is the LLM is based off of OpenChat-3.5-7B and is finetuned via RL using Starling-RM to produce state-of-the-art performance.

## Main Contribution

We provide the first high-quality 7-wise comparison dataset generated by GPT-4.  The 7 responses include mainly response we distilled from GPT-4, GPT-3.5-turbo, GPT-3.5-turbo-instruct, LLama-2-7B-chat, Mistral-7B-Instruct.  Responses were also sourced from other existing datasets.